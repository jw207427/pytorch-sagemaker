{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0b5f5a-1dd7-4307-ad03-eecaba197b7a",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with an RNN\n",
    "\n",
    "In this notebook, you'll implement a recurrent neural network that performs sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1198a576-5595-4a01-aaa8-7c9795242c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()                    # Set a default S3 bucket\n",
    "prefix = 'sentiment_rnn'\n",
    "\n",
    "# read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff5438-190a-49e0-8391-4ee84d29ddd4",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "baec5fa6-3957-44c3-bdc1-f373f4f5571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# get rid of punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "reviews_split = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1773824-aab3-40c8-bb9e-712672b6a04d",
   "metadata": {},
   "source": [
    "### Encoding the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb92f521-4002-4516-9ac0-58f764d2ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to use this import \n",
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "## use the dict to tokenize each review in reviews_split\n",
    "## store the tokenized reviews in reviews_ints\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693486d-d457-48bb-8d37-6a998680a0c1",
   "metadata": {},
   "source": [
    "**Test your processing code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffc9bba1-7cd9-4a56-820d-4e058251573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  74072\n",
      "\n",
      "Tokenized review: \n",
      " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
     ]
    }
   ],
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
    "print()\n",
    "\n",
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', reviews_ints[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4975cf-95d3-4f48-b618-8651471e6bca",
   "metadata": {},
   "source": [
    "### Encoding the labels\n",
    "\n",
    "Our labels are \"positive\" or \"negative\". To use these labels in our network, we need to convert them to 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42c2e13f-91fb-4bec-828e-d18bc893ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1=positive, 0=negative label conversion\n",
    "labels_split = labels.split('\\n')\n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287c4d0-2543-4256-89cb-cd6af04188ae",
   "metadata": {},
   "source": [
    "### Removing Outliers\n",
    "\n",
    "1. Getting rid of extremely long or short reviews; the outliers\n",
    "2. Padding/truncating the remaining data so that we have reviews of the same length.\n",
    "\n",
    "First, remove *any* reviews with zero length from the `reviews_ints` list and their corresponding label in `encoded_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b479eebd-e859-460e-8310-b95322090632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers:  25001\n",
      "Number of reviews after removing outliers:  25000\n"
     ]
    }
   ],
   "source": [
    "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
    "\n",
    "## remove any reviews/labels with zero length from the reviews_ints list.\n",
    "\n",
    "# get indices of any reviews with length 0\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "\n",
    "# remove 0-length reviews and their labels\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1676f64f-8e14-424b-8d9e-26c980533928",
   "metadata": {},
   "source": [
    "## Padding sequences\n",
    "\n",
    "To deal with both short and very long reviews, we'll pad or truncate all our reviews to a specific length. For reviews shorter than some `seq_length`, we'll pad with 0s. For reviews longer than `seq_length`, we can truncate them to the first `seq_length` words. A good `seq_length`, in this case, is 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "563fbc8c-a967-4052-9873-4e052567027c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
      " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   54    10    14   116    60   798   552    71   364     5]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   330   578    34     3   162   748  2731     9   325]\n",
      " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
      " [    1    20     6    76    40     6    58    81    95     5]\n",
      " [   54    10    84   329 26230 46427    63    10    14   614]\n",
      " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   40    26   109 17952  1422     9     1   327     4   125]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   10   499     1   307 10399    55    74     8    13    30]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Test your implementation!\n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features[:30,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4c2fb-d5f4-4c89-b557-e44add9322a3",
   "metadata": {},
   "source": [
    "## Split Training, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36318be7-8081-4c6e-8486-3dc0febd6d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2f1dbf4-2043-4648-840a-85b8f9a50c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.concatenate((train_x, \n",
    "                        train_y.reshape(train_y.shape[0], 1)),\n",
    "                       axis=1)\n",
    "valid = np.concatenate((val_x, \n",
    "                        val_y.reshape(val_y.shape[0], 1)),\n",
    "                       axis=1)\n",
    "test = np.concatenate((test_x, \n",
    "                        test_y.reshape(test_y.shape[0], 1)),\n",
    "                       axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d73c3c5d-887f-473a-aee8-a8517c59aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r data/processed\n",
    "!mkdir data/processed\n",
    "train_file = 'data/processed/train.txt'\n",
    "valid_file = 'data/processed/valid.txt'\n",
    "test_file = 'data/processed/test.txt'\n",
    "\n",
    "\n",
    "np.savetxt(train_file, train, delimiter=',')   # X is an array\n",
    "np.savetxt(valid_file, valid, delimiter=',')   # X is an arraynp.savetxt('train.out', train, delimiter=',')   # X is an array\n",
    "np.savetxt(test_file, test, delimiter=',')   # X is an array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc79dd-892a-407c-a4fe-2613b7e4c3c5",
   "metadata": {},
   "source": [
    "## test loading the data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "795883b8-5075-436d-a8a1-3fba0d5cbb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp32hiy5hs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.000e+00, 0.000e+00, 0.000e+00, ..., 4.500e+01, 4.000e+00,\n",
       "        1.000e+00],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 8.870e+02, 4.332e+03,\n",
       "        0.000e+00],\n",
       "       [9.273e+03, 4.300e+01, 4.650e+02, ..., 2.000e+00, 5.227e+03,\n",
       "        1.000e+00],\n",
       "       ...,\n",
       "       [4.600e+01, 1.100e+01, 6.000e+00, ..., 1.000e+00, 2.140e+02,\n",
       "        0.000e+00],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 9.980e+02, 2.700e+02,\n",
       "        1.000e+00],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 2.418e+03, 2.100e+01,\n",
       "        0.000e+00]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = np.loadtxt('data/processed/train.txt', delimiter=',')\n",
    "# train\n",
    "import io\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "f = open('data/processed/valid.txt', \"rb\")\n",
    "tfile = tempfile.NamedTemporaryFile(delete=False)\n",
    "tfile.write(f.read())\n",
    "\n",
    "print(tfile.name)\n",
    "test = np.loadtxt(tfile.name, delimiter=',')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50294797-b47c-4f6b-9814-c0d095f7e5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     8,   215,    23],\n",
       "       [    0,     0,     0, ...,    29,   108,  3324],\n",
       "       [22382,    42, 46418, ...,   483,    17,     3],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,    28,    77,   384],\n",
       "       [    0,     0,     0, ...,     1,  1893,  3610],\n",
       "       [    0,     0,     0, ...,     2,  2428,     8]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train[:, :-1]\n",
    "train_y = train[:, -1:].reshape(train.shape[0])\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5799698a-f86c-4fa8-94d9-c1f0fde6b3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74073"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816bd20-eba4-45e5-986e-cb8d210c06d1",
   "metadata": {},
   "source": [
    "### Upload file to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6bad9cb-857a-45f9-bbf2-7261040fc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3_data_path = dict()\n",
    "\n",
    "for f in [train_file, valid_file, test_file]:\n",
    "    filename = f.split('/')[-1]\n",
    "    object_name = f\"{prefix}/{filename}\"\n",
    "    s3.upload_file(f, bucket, object_name)\n",
    "    \n",
    "    s3_data_path[filename.split('.')[0]] = f\"s3://{bucket}/{object_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ce650-33c5-4a41-9d37-0b508b1c5616",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d28377b0-3009-4336-8337-3f981a6c695a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-987720697751/sentiment_rnn/train.txt'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f44a2235-9534-431e-8c05-85f269e3b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "hyperparameters = {\"epochs\": 4, \"batch_size\": 50} \n",
    "\n",
    "metric_definitions = [{'Name': 'Loss',      'Regex': 'Loss: ([0-9\\\\.]+)'},\n",
    "                      {'Name': 'Val_Loss',  'Regex': 'Val_Loss: ([0-9\\\\.]+)'},\n",
    "                      {'Name': 'val_loss',  'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "                      {'Name': 'val_acc',   'Regex': 'val_accuracy: ([0-9\\\\.]+)'}]\n",
    "\n",
    "\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"sentiment-rnn-pytorch\",\n",
    "    entry_point=\"sentiment_rnn.py\", \n",
    "    role=role,\n",
    "    framework_version=\"1.8.0\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    enable_sagemaker_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f7d467d-1784-464a-a073-a73dbf33b502",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-09 18:28:48 Starting - Starting the training job...\n",
      "2022-07-09 18:29:15 Starting - Preparing the instances for trainingProfilerReport-1657391328: InProgress\n",
      ".........\n",
      "2022-07-09 18:30:37 Downloading - Downloading input data...\n",
      "2022-07-09 18:31:14 Training - Downloading the training image...........................\n",
      "2022-07-09 18:35:44 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-07-09 18:35:47,275 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-07-09 18:35:47,299 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-07-09 18:35:47,306 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-07-09 18:35:47,824 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"valid\": \"/opt/ml/input/data/valid\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 50,\n",
      "        \"epochs\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"valid\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sentiment-rnn-pytorch-2022-07-09-18-28-48-414\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-987720697751/sentiment-rnn-pytorch-2022-07-09-18-28-48-414/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sentiment_rnn\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sentiment_rnn.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":50,\"epochs\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sentiment_rnn.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"valid\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sentiment_rnn\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-987720697751/sentiment-rnn-pytorch-2022-07-09-18-28-48-414/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"valid\":\"/opt/ml/input/data/valid\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":50,\"epochs\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sentiment-rnn-pytorch-2022-07-09-18-28-48-414\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-987720697751/sentiment-rnn-pytorch-2022-07-09-18-28-48-414/source/sourcedir.tar.gz\",\"module_name\":\"sentiment_rnn\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sentiment_rnn.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"50\",\"--epochs\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALID=/opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=50\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 sentiment_rnn.py --batch_size 50 --epochs 4\u001b[0m\n",
      "\u001b[34mParsing command-line arguments...\u001b[0m\n",
      "\u001b[34mStart loading data...\u001b[0m\n",
      "\u001b[34mtraining data size is (20000, 200)\u001b[0m\n",
      "\u001b[34mvalidation data size is (2500, 200)\u001b[0m\n",
      "\u001b[34mStart training loop...\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.575 algo-1:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.703 algo-1:26 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.704 algo-1:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.704 algo-1:26 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.705 algo-1:26 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.705 algo-1:26 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.857 algo-1:26 INFO hook.py:584] name:embedding.weight count_params:29629200\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.857 algo-1:26 INFO hook.py:584] name:lstm.weight_ih_l0 count_params:409600\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.857 algo-1:26 INFO hook.py:584] name:lstm.weight_hh_l0 count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.857 algo-1:26 INFO hook.py:584] name:lstm.bias_ih_l0 count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.858 algo-1:26 INFO hook.py:584] name:lstm.bias_hh_l0 count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.858 algo-1:26 INFO hook.py:584] name:lstm.weight_ih_l1 count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.858 algo-1:26 INFO hook.py:584] name:lstm.weight_hh_l1 count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.858 algo-1:26 INFO hook.py:584] name:lstm.bias_ih_l1 count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.858 algo-1:26 INFO hook.py:584] name:lstm.bias_hh_l1 count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.858 algo-1:26 INFO hook.py:584] name:fc.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.858 algo-1:26 INFO hook.py:584] name:fc.bias count_params:1\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.858 algo-1:26 INFO hook.py:586] Total Trainable Params: 30829585\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.858 algo-1:26 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-07-09 18:35:58.860 algo-1:26 INFO hook.py:476] Hook is writing from the hook with pid: 26\u001b[0m\n",
      "\u001b[34mEpoch: 1/4...\u001b[0m\n",
      "\u001b[34mStep: 100...\u001b[0m\n",
      "\u001b[34mLoss: 0.6465958952903748...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.6597788918018341\u001b[0m\n",
      "\u001b[34mEpoch: 1/4...\u001b[0m\n",
      "\u001b[34mStep: 200...\u001b[0m\n",
      "\u001b[34mLoss: 0.6317886114120483...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.6466264450550079\u001b[0m\n",
      "\u001b[34mEpoch: 1/4...\u001b[0m\n",
      "\u001b[34mStep: 300...\u001b[0m\n",
      "\u001b[34mLoss: 0.6798592209815979...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.5683226674795151\u001b[0m\n",
      "\u001b[34mEpoch: 1/4...\u001b[0m\n",
      "\u001b[34mStep: 400...\u001b[0m\n",
      "\u001b[34mLoss: 0.653045117855072...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.854876629114151\u001b[0m\n",
      "\u001b[34mEpoch: 2/4...\u001b[0m\n",
      "\u001b[34mStep: 500...\u001b[0m\n",
      "\u001b[34mLoss: 0.6479055881500244...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.6439543962478638\u001b[0m\n",
      "\u001b[34mEpoch: 2/4...\u001b[0m\n",
      "\u001b[34mStep: 600...\u001b[0m\n",
      "\u001b[34mLoss: 0.3976552486419678...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.6766604375839234\u001b[0m\n",
      "\u001b[34mEpoch: 2/4...\u001b[0m\n",
      "\u001b[34mStep: 700...\u001b[0m\n",
      "\u001b[34mLoss: 0.2960318326950073...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.5043764072656631\u001b[0m\n",
      "\u001b[34mEpoch: 2/4...\u001b[0m\n",
      "\u001b[34mStep: 800...\u001b[0m\n",
      "\u001b[34mLoss: 0.3940376937389374...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.43405424982309343\u001b[0m\n",
      "\u001b[34mEpoch: 3/4...\u001b[0m\n",
      "\u001b[34mStep: 900...\u001b[0m\n",
      "\u001b[34mLoss: 0.35317468643188477...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.43752642929553986\u001b[0m\n",
      "\u001b[34mEpoch: 3/4...\u001b[0m\n",
      "\u001b[34mStep: 1000...\u001b[0m\n",
      "\u001b[34mLoss: 0.16239146888256073...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.44728474259376527\u001b[0m\n",
      "\u001b[34mEpoch: 3/4...\u001b[0m\n",
      "\u001b[34mStep: 1100...\u001b[0m\n",
      "\u001b[34mLoss: 0.2757677733898163...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.43339626729488373\u001b[0m\n",
      "\u001b[34mEpoch: 3/4...\u001b[0m\n",
      "\u001b[34mStep: 1200...\u001b[0m\n",
      "\u001b[34mLoss: 0.2808299958705902...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.4274515968561172\u001b[0m\n",
      "\u001b[34mEpoch: 4/4...\u001b[0m\n",
      "\u001b[34mStep: 1300...\u001b[0m\n",
      "\u001b[34mLoss: 0.21681630611419678...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.4556403186917305\u001b[0m\n",
      "\u001b[34mEpoch: 4/4...\u001b[0m\n",
      "\u001b[34mStep: 1400...\u001b[0m\n",
      "\u001b[34mLoss: 0.254268079996109...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.5001518255472184\u001b[0m\n",
      "\u001b[34mEpoch: 4/4...\u001b[0m\n",
      "\u001b[34mStep: 1500...\u001b[0m\n",
      "\u001b[34mLoss: 0.2789299190044403...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.4784656304121018\u001b[0m\n",
      "\u001b[34mEpoch: 4/4...\u001b[0m\n",
      "\u001b[34mStep: 1600...\u001b[0m\n",
      "\u001b[34mLoss: 0.13153281807899475...\u001b[0m\n",
      "\u001b[34mVal_Loss: 0.47206099838018417\u001b[0m\n",
      "\u001b[34mTraining complete successfully....\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mTraining complete...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 1/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 100...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.6465958952903748...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.6597788918018341\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 1/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 200...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.6317886114120483...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.6466264450550079\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 1/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 300...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.6798592209815979...\u001b[0m\n",
      "\u001b[34m2022-07-09 18:37:06,574 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.5683226674795151\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 1/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 400...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.653045117855072...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.854876629114151\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 2/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 500...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.6479055881500244...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.6439543962478638\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 2/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 600...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.3976552486419678...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.6766604375839234\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 2/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 700...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.2960318326950073...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.5043764072656631\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 2/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 800...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.3940376937389374...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.43405424982309343\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 3/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 900...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.35317468643188477...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.43752642929553986\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 3/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 1000...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.16239146888256073...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.44728474259376527\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 3/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 1100...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.2757677733898163...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.43339626729488373\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 3/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 1200...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.2808299958705902...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.4274515968561172\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 4/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 1300...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.21681630611419678...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.4556403186917305\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 4/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 1400...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.254268079996109...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.5001518255472184\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 4/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 1500...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.2789299190044403...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.4784656304121018\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epoch: 4/4...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Step: 1600...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Loss: 0.13153281807899475...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Val_Loss: 0.47206099838018417\u001b[0m\n",
      "\u001b[34mINFO:__main__:Training complete successfully....\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Training complete...\u001b[0m\n",
      "\n",
      "2022-07-09 18:37:15 Uploading - Uploading generated training model\n",
      "2022-07-09 18:37:55 Completed - Training job completed\n",
      "ProfilerReport-1657391328: NoIssuesFound\n",
      "Training seconds: 424\n",
      "Billable seconds: 424\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"train\": s3_data_path['train'], \n",
    "          \"valid\": s3_data_path['valid']}\n",
    "\n",
    "estimator.fit(inputs, wait=True, logs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dba73126-faff-4e1c-bdac-c48170ea60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "inference_prefix = \"batch_transform\"\n",
    "\n",
    "model_artifact_s3_location = estimator.model_data  # \"s3://<BUCKET>/<PREFIX>/model.tar.gz\"\n",
    "s3_output_path = f\"s3://{bucket}/{prefix}/{inference_prefix}\"\n",
    "# !aws s3 cp $model_artifact_s3_location model.tar.gz\n",
    "# !tar -xf model.tar.gz\n",
    "\n",
    "# Create PyTorchModel from saved model artifact\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_artifact_s3_location,\n",
    "    role=role,\n",
    "    framework_version=\"1.8.0\",\n",
    "    py_version=\"py3\",\n",
    "    entry_point=\"sentiment_rnn.py\",\n",
    ")\n",
    "\n",
    "transformer = pytorch_model.transformer(instance_count=1, \n",
    "                                        instance_type=\"ml.c5.xlarge\",\n",
    "                                        output_path = s3_output_path,\n",
    "                                        assemble_with = 'Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5fd003d-d926-489b-bec3-22f2694cf451",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................\u001b[34m2022-07-09 19:31:37,788 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.3.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 938 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model.mar\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:37,788 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[35mTorchserve version: 0.3.0\u001b[0m\n",
      "\u001b[35mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[35mCurrent directory: /\u001b[0m\n",
      "\u001b[35mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[35mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[35mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[35mMax heap size: 938 M\u001b[0m\n",
      "\u001b[35mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[35mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[35mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[35mInitial Models: model.mar\u001b[0m\n",
      "\u001b[35mLog dir: /logs\u001b[0m\n",
      "\u001b[35mMetrics dir: /logs\u001b[0m\n",
      "\u001b[35mNetty threads: 0\u001b[0m\n",
      "\u001b[35mNetty client threads: 0\u001b[0m\n",
      "\u001b[35mDefault workers per model: 4\u001b[0m\n",
      "\u001b[35mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[35mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[35mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[35mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:37,820 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,632 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 97115d673b734800ace1e0d12d907659\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,642 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,672 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,827 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,827 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,829 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]44\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,830 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]43\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,830 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,832 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,833 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,834 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,840 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,841 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,848 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,849 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]48\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,849 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,849 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,849 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,901 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,904 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,910 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,911 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,911 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,913 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,915 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,915 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]49\u001b[0m\n",
      "\u001b[35mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[35mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[35mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[35mEnable metrics API: true\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:37,820 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,632 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 97115d673b734800ace1e0d12d907659\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,642 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,672 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,827 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,827 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,829 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]44\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,830 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]43\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,830 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,832 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,833 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,834 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,840 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,841 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,848 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,849 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]48\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,849 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,849 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,849 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,901 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,904 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,910 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,911 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,911 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,913 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,915 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,915 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]49\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,916 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,916 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,916 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:39,920 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:40,175 [INFO ] pool-1-thread-5 ACCESS_LOG - /169.254.255.130:40742 \"GET /ping HTTP/1.1\" 200 41\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:40,176 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:40,308 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:40748 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:40,312 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:40,591 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:40,592 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:47.45648193359375|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:40,593 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:8.459430694580078|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:40,593 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:15.1|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:40,594 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6057.8671875|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:40,595 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1280.8984375|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:40,595 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:20.5|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,916 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,916 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,916 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:39,920 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:40,175 [INFO ] pool-1-thread-5 ACCESS_LOG - /169.254.255.130:40742 \"GET /ping HTTP/1.1\" 200 41\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:40,176 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:40,308 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:40748 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:40,312 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[35mModel server started.\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:40,591 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:40,592 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:47.45648193359375|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:40,593 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:8.459430694580078|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:40,593 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:15.1|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:40,594 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6057.8671875|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:40,595 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1280.8984375|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:40,595 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:20.5|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395100\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:41,868 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1838\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:41,868 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:2218|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395101\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:41,868 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:110|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:41,906 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1876\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:41,907 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:2257|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395101\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:41,907 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:111|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:41,868 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1838\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:41,868 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:2218|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395101\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:41,868 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:110|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:41,906 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1876\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:41,907 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:2257|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395101\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:41,907 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:111|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:41,977 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1948\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:41,977 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:2327|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395101\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:41,977 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:109|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:42,051 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2022\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:42,051 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:2403|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395102\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:42,051 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:109|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:41,977 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1948\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:41,977 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:2327|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395101\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:41,977 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:109|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:42,051 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2022\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:42,051 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:2403|#Level:Host|#hostname:1ee72ce727e5,timestamp:1657395102\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:42,051 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:109|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[32m2022-07-09T19:31:40.336:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:46,798 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - custom output function.....\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:46,798 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - output list length\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:46,798 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 1252\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:46,798 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4694\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:46,799 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:40770 \"POST /invocations HTTP/1.1\" 200 4714\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:46,799 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:46,799 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:46,799 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:46,798 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:4687.74|#ModelName:model,Level:Model|#hostname:1ee72ce727e5,requestID:04c08786-c559-4433-a104-107b53b4500b,timestamp:1657395106\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:46,798 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - custom output function.....\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:46,798 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - output list length\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:46,798 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 1252\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:46,798 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4694\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:46,799 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:40770 \"POST /invocations HTTP/1.1\" 200 4714\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:46,799 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:46,799 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:46,799 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:46,798 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:4687.74|#ModelName:model,Level:Model|#hostname:1ee72ce727e5,requestID:04c08786-c559-4433-a104-107b53b4500b,timestamp:1657395106\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4081\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - custom input function.....\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - /home/model-server/tmp/tmp55ch1q22\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Input numpy array size (1248, 200)....\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - custom predict function.....\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Input data shape (1248, 200) and label shape (1248,)...\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Prediction data shape (1248,)....\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - custom output function.....\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - output list length\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 1248\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:40770 \"POST /invocations HTTP/1.1\" 200 4113\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,025 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,025 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:4079.1|#ModelName:model,Level:Model|#hostname:1ee72ce727e5,requestID:b90a338a-dae1-4338-b571-4db4a0f2448d,timestamp:1657395111\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,025 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-07-09 19:31:51,025 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:14|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4081\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - custom input function.....\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - /home/model-server/tmp/tmp55ch1q22\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Input numpy array size (1248, 200)....\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - custom predict function.....\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Input data shape (1248, 200) and label shape (1248,)...\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Prediction data shape (1248,)....\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - custom output function.....\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - output list length\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - 1248\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,024 [INFO ] W-9001-model_1 ACCESS_LOG - /169.254.255.130:40770 \"POST /invocations HTTP/1.1\" 200 4113\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,025 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,025 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:4079.1|#ModelName:model,Level:Model|#hostname:1ee72ce727e5,requestID:b90a338a-dae1-4338-b571-4db4a0f2448d,timestamp:1657395111\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,025 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-07-09 19:31:51,025 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:14|#Level:Host|#hostname:1ee72ce727e5,timestamp:null\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer.transform(\n",
    "    data=s3_data_path['test'],\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"text/plain\",\n",
    "    split_type='Line',\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c1605c-6cb6-4865-9572-34d9202ec150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_bucket_and_prefix(s3_output_path):\n",
    "    trim = re.sub(\"s3://\", \"\", s3_output_path)\n",
    "    bucket, prefix = trim.split(\"/\")\n",
    "    return bucket, prefix\n",
    "\n",
    "\n",
    "local_path = \"output\"  # Where to save the output locally\n",
    "\n",
    "bucket, output_prefix = get_bucket_and_prefix(s3_output_path)\n",
    "print(bucket, output_prefix)\n",
    "\n",
    "sagemaker_session.download_data(path=local_path, bucket=bucket, key_prefix=output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54282fe4-ac14-4f82-844c-5a73d1e18e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for f in os.listdir(local_path):\n",
    "    path = os.path.join(local_path, f)\n",
    "    with open(path, \"r\") as f:\n",
    "        pred = json.load(f)\n",
    "        print(pred)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
